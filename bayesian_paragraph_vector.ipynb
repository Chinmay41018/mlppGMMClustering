{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro \n",
    "import torch \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "from torch.nn import Embedding\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('./data/cleaned_documents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 100 # size of embedding\n",
    "sent = []\n",
    "for each in dat.text:\n",
    "    sent = sent + ast.literal_eval(each)\n",
    "W = len(np.unique(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15673\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= np.unique(sent)\n",
    "word2idx = {word:idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx:word for idx, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in uni_freq:\n",
    "    uni_freq[each] = uni_freq[each]**0.75\n",
    "deno = sum(uni_freq.values())\n",
    "for each in uni_freq:\n",
    "    uni_freq[each] = uni_freq[each]/deno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each document in the corpus, we get x_n^+ and x_n^- by looking at context window and negative sampling. \n",
    "Ref : https://arxiv.org/pdf/1711.03946.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coding context window size to 3.\n",
    "c = 2\n",
    "neg_samples = 2*c\n",
    "def get_pos_neg_sample(dat):\n",
    "    x_plus = []\n",
    "    x_neg = []\n",
    "    uni_keys = list(uni_freq.keys())\n",
    "    uni_values = list(uni_freq.values())\n",
    "    for each in dat.text:\n",
    "        x_plus_n = []\n",
    "        x_neg_n = []\n",
    "        sentence = ast.literal_eval(each)\n",
    "        random_sample = np.random.choice(uni_keys, len(sentence)*neg_samples, p = uni_values)\n",
    "        r_i = 0\n",
    "        for idx, token in enumerate(sentence):\n",
    "            context = sentence[max(idx-c,0):min(idx+c+1, len(sentence))]\n",
    "            context.remove(token)\n",
    "            for every in context:\n",
    "                x_plus_n.append((word2idx[token], word2idx[every]))\n",
    "                \n",
    "            for sample in random_sample[r_i:r_i+len(context)]:\n",
    "                x_neg_n.append((word2idx[token], word2idx[sample]))\n",
    "            r_i+=len(context)\n",
    "            \n",
    "        x_plus.append(x_plus_n)\n",
    "        x_neg.append(x_neg_n)\n",
    "    return x_plus, x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plus, x_neg = get_pos_neg_sample(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 : Update U,V using sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  1\n",
      "tensor(-1111.8203, grad_fn=<SubBackward0>) tensor(-1.7039e-06, grad_fn=<MeanBackward0>) tensor(-2.2608e-08)\n",
      "Iteration :  11\n",
      "tensor(-474.1290, grad_fn=<SubBackward0>) tensor(-1.6813e-06, grad_fn=<MeanBackward0>) tensor(-1.7723e-10)\n",
      "Iteration :  21\n",
      "tensor(-1716.2916, grad_fn=<SubBackward0>) tensor(-1.6793e-06, grad_fn=<MeanBackward0>) tensor(1.9188e-10)\n",
      "Iteration :  31\n",
      "tensor(-867.8243, grad_fn=<SubBackward0>) tensor(-1.6800e-06, grad_fn=<MeanBackward0>) tensor(-1.1165e-10)\n",
      "Iteration :  41\n",
      "tensor(-463.0487, grad_fn=<SubBackward0>) tensor(-1.6802e-06, grad_fn=<MeanBackward0>) tensor(-1.8893e-10)\n",
      "Iteration :  51\n",
      "tensor(-629.3931, grad_fn=<SubBackward0>) tensor(-1.6795e-06, grad_fn=<MeanBackward0>) tensor(7.1556e-11)\n",
      "Iteration :  61\n",
      "tensor(-707.0316, grad_fn=<SubBackward0>) tensor(-1.6789e-06, grad_fn=<MeanBackward0>) tensor(5.9516e-11)\n",
      "Iteration :  71\n",
      "tensor(-529.5614, grad_fn=<SubBackward0>) tensor(-1.6779e-06, grad_fn=<MeanBackward0>) tensor(-9.3129e-11)\n",
      "Iteration :  81\n",
      "tensor(-745.8411, grad_fn=<SubBackward0>) tensor(-1.6772e-06, grad_fn=<MeanBackward0>) tensor(2.0906e-10)\n",
      "Iteration :  91\n",
      "tensor(-1804.9880, grad_fn=<SubBackward0>) tensor(-1.6785e-06, grad_fn=<MeanBackward0>) tensor(-2.1648e-10)\n",
      "Iteration :  101\n",
      "tensor(-745.8245, grad_fn=<SubBackward0>) tensor(-1.6781e-06, grad_fn=<MeanBackward0>) tensor(6.6165e-11)\n",
      "Iteration :  111\n",
      "tensor(-690.3915, grad_fn=<SubBackward0>) tensor(-1.6780e-06, grad_fn=<MeanBackward0>) tensor(-1.7831e-10)\n",
      "Iteration :  121\n",
      "tensor(-1355.7983, grad_fn=<SubBackward0>) tensor(-1.6775e-06, grad_fn=<MeanBackward0>) tensor(2.9899e-10)\n",
      "Iteration :  131\n",
      "tensor(-895.5799, grad_fn=<SubBackward0>) tensor(-1.6777e-06, grad_fn=<MeanBackward0>) tensor(1.3368e-10)\n",
      "Iteration :  141\n",
      "tensor(-973.1868, grad_fn=<SubBackward0>) tensor(-1.6785e-06, grad_fn=<MeanBackward0>) tensor(-1.6981e-10)\n",
      "Iteration :  151\n",
      "tensor(-695.9137, grad_fn=<SubBackward0>) tensor(-1.6780e-06, grad_fn=<MeanBackward0>) tensor(2.7058e-11)\n",
      "Iteration :  161\n",
      "tensor(-585.0428, grad_fn=<SubBackward0>) tensor(-1.6789e-06, grad_fn=<MeanBackward0>) tensor(-2.7004e-10)\n",
      "Iteration :  171\n",
      "tensor(-1383.5027, grad_fn=<SubBackward0>) tensor(-1.6789e-06, grad_fn=<MeanBackward0>) tensor(1.3128e-10)\n",
      "Iteration :  181\n",
      "tensor(-701.4951, grad_fn=<SubBackward0>) tensor(-1.6783e-06, grad_fn=<MeanBackward0>) tensor(-2.2648e-10)\n",
      "Iteration :  191\n",
      "tensor(-645.9935, grad_fn=<SubBackward0>) tensor(-1.6785e-06, grad_fn=<MeanBackward0>) tensor(2.7214e-10)\n",
      "Iteration :  201\n",
      "tensor(-1195.0309, grad_fn=<SubBackward0>) tensor(-1.6792e-06, grad_fn=<MeanBackward0>) tensor(-3.6572e-11)\n",
      "Iteration :  211\n",
      "tensor(-1145.0789, grad_fn=<SubBackward0>) tensor(-1.6797e-06, grad_fn=<MeanBackward0>) tensor(1.2051e-11)\n",
      "Iteration :  221\n",
      "tensor(-984.3210, grad_fn=<SubBackward0>) tensor(-1.6796e-06, grad_fn=<MeanBackward0>) tensor(1.1772e-10)\n",
      "Iteration :  231\n",
      "tensor(-712.5633, grad_fn=<SubBackward0>) tensor(-1.6790e-06, grad_fn=<MeanBackward0>) tensor(1.1702e-10)\n",
      "Iteration :  241\n",
      "tensor(-1261.5645, grad_fn=<SubBackward0>) tensor(-1.6793e-06, grad_fn=<MeanBackward0>) tensor(-2.0150e-10)\n",
      "Iteration :  251\n",
      "tensor(-845.6448, grad_fn=<SubBackward0>) tensor(-1.6783e-06, grad_fn=<MeanBackward0>) tensor(1.7413e-10)\n",
      "Iteration :  261\n",
      "tensor(-962.0903, grad_fn=<SubBackward0>) tensor(-1.6790e-06, grad_fn=<MeanBackward0>) tensor(6.2243e-11)\n",
      "Iteration :  271\n",
      "tensor(-1106.3008, grad_fn=<SubBackward0>) tensor(-1.6783e-06, grad_fn=<MeanBackward0>) tensor(5.7927e-11)\n"
     ]
    }
   ],
   "source": [
    "lamb = 0.01\n",
    "phi = 0.01\n",
    "annealed_rate = 0.0001\n",
    "alpha = 0.999\n",
    "n_iter = 100\n",
    "loss = []\n",
    "U = torch.distributions.MultivariateNormal(torch.zeros(W,E), lamb**2 *torch.eye(E)).rsample()\n",
    "V = torch.distributions.MultivariateNormal(torch.zeros(W,E), lamb**2 *torch.eye(E)).rsample()\n",
    "U.requires_grad=True\n",
    "V.requires_grad=True\n",
    "for n in range(len(x_plus)):\n",
    "    annealed_rate = annealed_rate * alpha\n",
    "    d_i = torch.distributions.MultivariateNormal( torch.zeros(E), phi*torch.eye(E)).rsample()\n",
    "    d_i.requires_grad=True\n",
    "\n",
    "    X_plus = torch.tensor(x_plus[n])\n",
    "    X_neg = torch.tensor(x_neg[n])\n",
    "    #SGD for d_i\n",
    "    for iterat in range(n_iter):\n",
    "        # Loss function\n",
    "        logProb = -torch.sum(torch.log(1+torch.exp(-torch.bmm(U[X_plus[:,0]].view(U[X_plus[:,0]].shape[0], \n",
    "                                                                                  1,\n",
    "                                                                                  E) ,\n",
    "                                                              (V[X_plus[:,1]]+d_i).view((V[X_plus[:,1]]+d_i).shape[0],\n",
    "                                                                                        E,\n",
    "                                                                                        1)))))\n",
    "        logProb -= torch.sum(torch.log(1+torch.exp(-torch.bmm(-U[X_neg[:,0]].view(U[X_neg[:,0]].shape[0], \n",
    "                                                                                  1,\n",
    "                                                                                  E) ,\n",
    "                                                              (V[X_neg[:,1]]+d_i).view((V[X_neg[:,1]]+d_i).shape[0],\n",
    "                                                                                        E,\n",
    "                                                                                        1)))))\n",
    "\n",
    "\n",
    "        d_i.retain_grad()\n",
    "        logProb.backward()\n",
    "        d_i = d_i-0.0001*d_i.grad\n",
    "        \n",
    "    # SGD for U_i, V_j\n",
    "    logProb = -torch.sum(torch.log(1+torch.exp(-torch.bmm(U[X_plus[:,0]].view(U[X_plus[:,0]].shape[0], \n",
    "                                                                              1,\n",
    "                                                                              E) ,\n",
    "                                                          (V[X_plus[:,1]]+d_i).view((V[X_plus[:,1]]+d_i).shape[0],\n",
    "                                                                                    E,\n",
    "                                                                                    1)))))\n",
    "    logProb -= torch.sum(torch.log(1+torch.exp(-torch.bmm(-U[X_neg[:,0]].view(U[X_neg[:,0]].shape[0], \n",
    "                                                                              1,\n",
    "                                                                              E) ,\n",
    "                                                          (V[X_neg[:,1]]+d_i).view((V[X_neg[:,1]]+d_i).shape[0],\n",
    "                                                                                    E,\n",
    "                                                                                    1)))))\n",
    "\n",
    "    loss.append(logProb)\n",
    "    U.retain_grad()\n",
    "    V.retain_grad()\n",
    "    logProb.backward()\n",
    "    if n%10==0:\n",
    "        print(\"Iteration : \", n+1)\n",
    "        print(logProb, torch.mean(U), torch.mean(annealed_rate * U.grad))\n",
    "    U = U - annealed_rate * U.grad\n",
    "    V = V - annealed_rate * V.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(345.2450, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(1+torch.exp(-torch.bmm(-U[X_neg[:,0]].view(U[X_neg[:,0]].shape[0], \n",
    "                                                                              1,\n",
    "                                                                              E) ,\n",
    "                                                          (V[X_neg[:,1]]+d_i).view((V[X_neg[:,1]]+d_i).shape[0],\n",
    "                                                                                    E,\n",
    "                                                                                    1)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(345.2450, grad_fn=<TraceBackward>)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(torch.log(1+torch.exp(-torch.mm(-U[X_neg[:,0]], (V[X_neg[:,1]]+d_i).t()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_plus,X_neg):\n",
    "    with pyro.plate('component', W):\n",
    "        lamb = 5\n",
    "        U = pyro.sample('U', dist.MultivariateNormal(loc = torch.zeros(E), scale = lamb*torch.eye(E)), requires_gradient=True)\n",
    "        V = pyro.sample('V', dist.MultivariateNormal(loc = torch.zeros(E), scale = lamb*torch.eye(E)), requires_gradient=True)\n",
    "    phi = 10\n",
    "    d_i = pyro.sample('d_i', dist.MultivariateNormal(loc = torch.zeros(E), scale = phi*torch.eye(E)),requires_gradient=True)\n",
    "                                          \n",
    "    dfx, = grad(logProb,d_i,create_graph=True)\n",
    "    for i in range(100):\n",
    "        # Loss function\n",
    "        logProb = torch.log(torch.sigmoid(torch.dot(U[X_plus[0][0]],V[X_plus[0][1]]+d_i))\n",
    "        for iteration in range(1,len(X_plus)):\n",
    "            i,j = X_plus[iteration]\n",
    "            logProb += torch.log(torch.sigmoid(torch.dot(U[i],V[j]+d_i))\n",
    "        for i,j in X_neg:\n",
    "            logProb += torch.log(torch.sigmoid(torch.dot(-U[i],V[j]+d_i))\n",
    "        logProb.backward()\n",
    "        with torch.no_grad():\n",
    "            d_i = d_i-0.001*d_i.grad \n",
    "    pred \n",
    "    \n",
    "    with pyro.plate('data', len(data)):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logProb = torch.Value(0,requires_gradient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Fix U,V and update d (doc vector) using SVI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
